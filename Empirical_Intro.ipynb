{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Empirical - Intro.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maAYVbGQaF5K"
      },
      "source": [
        "# Generating qualitative models of observational data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccg5l7Lr7sJE"
      },
      "source": [
        "This example focuses on empirical system engineering by extracting engineering models from observation data assisted by reasoning.\n",
        "\n",
        "The design and operation of modern IT-based systems, like info-communication infrastructures, or cyber-physical systems need model-based approaches due to their complexity. \n",
        "\n",
        "However, the limited faithfulness of pure analytic models prohibits their use in complex systems. This way complexity necessitates empirical system identification; i.e the extraction of models describing quantitative attributes, like performance, throughput, timeliness, dependability, etc. from observations (benchmarks, operation logs).\n",
        "\n",
        "The main approach to systems identification is **exploratory data analysis** (EDA) carried out by a domain expert. \n",
        "\n",
        "This process relies on a combination of visual methods and summary statistics. EDA targets an initial analysis of data sets to reveal their main characteristics, to discover typical patterns, to abstract and generalize them in the form of model fragments, and to identify outliers, etc. \n",
        "\n",
        "The main difficulties in traditional EDA are twofold: \n",
        "\n",
        "\n",
        "1.   the expert usually evaluates only a *fragment of  the many-dimensional big observation data* and he has to synthesize the *global model* from these fragments;\n",
        "2.    the mathematical model has to put into the context of the engineering model by *model fusion*. \n",
        "\n",
        "The **initial engineering model** describes here the apriori knowledge on the system, like *physical constraints* on data, *system architecture*, *data flow*, and information on *causal (in)dependencies*. \n",
        "\n",
        "The purposes of the joint management of these two models are \n",
        "*   the empirical *validation* and *refinement* of the engineering model; \n",
        "*   *guiding the exploration process* in EDA, and \n",
        "*   *checking the correctness* of the observation by evaluating their *consistency* with the apriori knowledge.\n",
        "\n",
        "The main difficulties related to the traditional EDA process originate in the lack of formal methods based support for the joint handling of the engineering and statistical models. \n",
        "\n",
        "This way the interpretation, validation, etc. of these models are left to a pure human task to be performed by the domain expert.\n",
        "\n",
        "The core idea is the representation of the mathematical model fragments and the engineering model uniformly abstract discrete (qualitative)models.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqDMEubMcptu"
      },
      "source": [
        "# 1.Initial hypothesis and model building from observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feg6jnXabm64"
      },
      "source": [
        "## 1.1 Objective\n",
        "\n",
        "The introductory lectures present the initial phase of **system identification**.\n",
        "\n",
        "Following the initial **exploratory analysis** of the representative sample dataset highlighting the focal problems, a detailed analysis phase targets the creation of **valid observations** over the target system.\n",
        "\n",
        "This way the output of this system identification is a set of phenomenological **model hypothesis** (like statistical models) provably valid over the sample dataset (AI refers to the subset as the training set). A subsequent **confirmatory analysis** checks the validity of the hypotheses over the remaining data not analyzed yet (checking set in the terminology of artificial intelligence)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmfC4Dxacsx8"
      },
      "source": [
        "## 1.2 The EDA process\n",
        "\n",
        "1. At first, the process called  **extract, transform, load**  (**ETL**) collects data from different sources and transforms them into a format supporting visual and statistical analysis.\n",
        "\n",
        "1. A highly  **informal data model** (for simplicity in the form of a mindmap in our case) is the first work item created. It illustrates the structure of the data records, the potential hierarchy of notions and data types. Subsequently, the expert identifies the fundamental requirements needed by the data set prior to analysis.\n",
        "2. The optional step of **data fusion** merges data from different sources. \n",
        "3. **Data cleansing** or  **data wrangling** decide upon the mitigation actions related to missing, ill-formulated or otherwise faulty data and performs the actions to consolidate the dataset to a ready-for-analysis state.\n",
        "4. **Early visual exploratory data analysis (EDA)** serves the preliminary assessment of the dataset and potentially highlights interesting focus phenomena for the subsequent detailed analysis steps.  Note that the purpose of the early exploratory analysis phases is solely to get an insight into the features of the dataset with no intention of elaborating a model. Accordingly, most tools focus on usability only and pay little attention to reproducibility of the analysis process. This way the first findings serve only for a global, but shallow assessment by the expert. The detailed analyses is the subject of the later phases\n",
        "6. Modern data analysis frameworks facilitate exploratory analyses by means of automated **data profiling** , a general-purpose stored workflow. Data profiling performs the most typical application-independent statistical analysis tasks over the target dataset. The aspects covered include the overall characteristics of the dataset, like the most important measures of data quality, the distribution of the individual variables, and correlation analysis between pairs of variables.\n",
        "8. The so-called **drill-down** process separates the subdomains in the data space showing these phenomena and evaluates of the relevant subsets of data. As this phase serves as the basis of the elaboration of phenomenological system models as hypotheses, repeatability and reproducibility are of primary importance.  Nowadays experts use so-called statistical notebooks typically for this purpose.\n",
        "9. **Outlier detection** serves the identification of extraordinary cases deserving special attention in critical systems, like CPS. \n",
        "10. The final phase of initial system identification is **confirmatory analysis**, validating the previously extracted hypotheses by means of statistical examinations and probes over the original sample dataset.  \n",
        "Exploratory and confirmatory analysis form an iterative process identifying interesting phenomena, formulating them in the form of initial hypotheses and checking them by detailed exploratory analysis.\n",
        "\n",
        "11. **Automated checks** complement the workflow of exploratory data analysis by validating the initial hypotheses created over the sample dataset (referred in AI as *training set*) by applying and checking them over the additional datasets (*checking set*). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3AvE0PKuTPi"
      },
      "source": [
        "## 1.3 Running examples\n",
        "\n",
        "This section presents a very short description of the individual pilot examples.\n",
        "\n",
        "As data analysis is a highly practical field of data engineering, solving examples is the driving force for the lectures.\n",
        "\n",
        "Three examples represent the different aspects of data analysis (the  original, transformed and whenever possible the evaluated data are included into the respective webpage):\n",
        "The **NANO** (nanosatellite) database, the **CLOUD** round-trip execution time measurement, and the static analysis results of **LINUX** (Linux Kernel static analysis) served as pilot examples.\n",
        "\n",
        "1. The **NANO** (nanosatellite) example represents analysis of data originating in a manual entry process. While the so-called database looks sound at first glance, it consists of numerous ambiguities, confused data fields, content represented by a string.\n",
        "2. Measurement data [romanovsky.client.data.cleaned.csv] of a scientific experiment (described in details in [1]) substantiate the **CLOUD** example. This dataset served as a basis for a visual exploratory data analysis (EDA) tutorial [2].\n",
        "3. The **LINUX** dataset originates in a snapshot of the debugging results of the static analysis (Coverity) of the actual Linux core under development. Here no detailed knowledge of the analysis process is needed, but it is worth to look at the typical programming errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMMh7mAazjVY"
      },
      "source": [
        "## 1.4 Tools\n",
        "\n",
        "Open source tools served as a basic as a technical background. \n",
        "\n",
        "Stand-alone applications formed the core toolset in the course due to their better usability.  However, the individual sub-functionalities provided by the individual tools appear integrated in all modern (business) analytics environments.\n",
        "\n",
        "1. **Data acquisition:** In particular, tables on web pages were sources of data in the two pilot examples **NANO** and **LINUX**. \"Cut and paste\" to Excel transformed these datasets into a processable form. Excel was the environment for executing some straightforward manipulations, as well. The **CLOUD** dataset is available as .csv, the format understood by all tools in the field.\n",
        "\n",
        "2. **Informal data model:** My preference is using [XMind](https://www.xmind.net/) for a sketching of structure of the dataset, but this is a question of taste.\n",
        "3. **Data fusion and ETL:** [OpenRefine](https://openrefine.org/) provided the main data merging and cleansing environment. (The web page of the tool offers some good embedded video tutorials). Pay attention to resolving the ambiguities by manual and AI-supported clustering in the **NANO ** example (Torino!), and to splitting merged semantic content represented by strings into separate fields.\n",
        "\n",
        "4. **Visual EDA:** [Mondrian](http://www.theusrus.de/Mondrian/)(old but good, the stand-alone version of the _iplots_ package in R) facilitated the fast, preparatory visual EDA. (the web page has a sufficient guide for working with it, for further details see O[3]). \n",
        "Note, that by right clicking into the diagrams you get interesting manipulation features.\n",
        "\n",
        "3. **Data analysis:** All the further analysis task used python over a [Jupyter](https://jupyter.org/) notebook, which has probably the highest penetration in statistical analysis. \n",
        "The attached notebook for the CLOUD example may serve –naturally after a proper adaptation- as a start for a blueprint for the homework. \n",
        "The Jupyter web page consists a good documentation. As Jupyter is the mainstream technology, there is a large number of textual and video tutorials. The list of references consists only some links for the specific aspects of data profiling, exploratory analysis overview and extreme value analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFFikGzCf6gx"
      },
      "source": [
        "# 2.CLOUD example\n",
        "\n",
        "This notebook presents an end-to-end initial analysis process. Measurement data [romanovsky.client.data.cleaned.csv] of a scientific experiment (described in details in [1]) substantiate the CLOUD example. This dataset served as a basis for a visual exploratory data analysis (EDA) tutorial [2].\n",
        "![arch](https://drive.google.com/uc?id=1kXzX4BpxkGvoNUEzkP3aGwor5O4skn_I)\n",
        "\n",
        "---\n",
        "The measurement setup consists of clients running the WSsDAT initiating a Web Service at a remote server. \n",
        "\n",
        "Detailed description of the data:\n",
        "\n",
        "\n",
        "> The **client** at *location* is characterized by its *ip*, *client.type* and *country*. The identifier of the **server** is its *DC* data center name.\t\n",
        "\n",
        "> The **roundtrip time** *RT* has two components: the *RTT* (total) **transfer time** from the client to the server and return; and the *RPT* **processing time** at the server.\n",
        "\n",
        "> The dataset also contains **timestamps** for all the measurements as *start.time*. While the *pm.pa* variable represents the **day time** of the measurements, the *Time* variable represents the realtive **time zones**. \n",
        "\n",
        "\n",
        "M[1] A. Gorbenko, V. Kharchenko, S. Mamutov, O. Tarasyuk, and A. Romanovsky, \"Exploring Uncertainty of Delays as a Factor in End-to-End Cloud Response Time,\" 2012 Ninth European Dependable Computing Conference, Sibiu, 2012, pp. 185-190. https://eprint.ncl.ac.uk/190834\n",
        "\n",
        "M[2] Pataricza A., Kocsis I., Salánki Á., Gönczy L. (2013)  Empirical Assessment of Resilience. In: Gorbenko A., Romanovsky A., Kharchenko V. (eds) Software Engineering for Resilient Systems. SERENE 2013. LNCS-8166. Springer, Berlin, Heidelberg\n",
        "\n",
        "---\n",
        "\n",
        "2. Package documentations:\n",
        "\n",
        "- **[Pandas](https://pandas.pydata.org/docs/)** - Data analysis and manipulation tool.\n",
        "- **[Numpy](https://numpy.org/devdocs/user/quickstart.html)** - Mathematical library supports for large, multi-dimensional arrays and matrices.\n",
        "\n",
        "\n",
        "- **[Matplotlib](https://matplotlib.org/contents.html)** - Library for creating various visualizations in Python.\n",
        "- **[Seaborn](https://seaborn.pydata.org/)** - Statistical data vizualisation.\n",
        "- **[Plotly](https://plot.ly/python/)** - Interactive graphing library for Python.\n",
        "\n",
        "- **[scikit-learn](https://scikit-learn.org/)** - machine learning in Python.\n",
        "\n",
        "\n",
        "- **[Pandas profiling](https://github.com/pandas-profiling/pandas-profiling)** - Generates profile reports from a pandas DataFrame.\n",
        "\n",
        "\n",
        "- **[PyOD](https://pyod.readthedocs.io/en/latest/)** - Python toolkit for detecting outlying objects in multivariate data.\n",
        "\n",
        "- **[HiPlot](https://github.com/facebookresearch/hiplot)** - Lightweight interactive visualization tool to help discovering correlations and patterns in high-dimensional data.\n",
        "\n",
        "- **[DALEX](https://modeloriented.github.io/DALEX/)** - moDel Agnostic Language for Exploration and eXplanation. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAHBXLjZgutb"
      },
      "source": [
        "# **References**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVrMLiCKZLe1"
      },
      "source": [
        "\n",
        "M[1] A. Gorbenko, V. Kharchenko, S. Mamutov, O. Tarasyuk, and A. Romanovsky, &quot;[Exploring Uncertainty of Delays as a Factor in End-to-End Cloud Response Time](https://eprint.ncl.ac.uk/190834),&quot; 2012 Ninth European Dependable Computing Conference, Sibiu, 2012, pp. 185-190. [https://eprint.ncl.ac.uk/190834](https://eprint.ncl.ac.uk/190834)\n",
        "\n",
        "M[2] Pataricza A., Kocsis I., Salánki Á., Gönczy L. (2013)  [Empirical Assessment of Resilience](https://inf.mit.bme.hu/sites/default/files/publications/2013_SERENE_Pataricza-ag-ar_v2.pdf). In: Gorbenko A., Romanovsky A., Kharchenko V. (eds) Software Engineering for Resilient Systems. SERENE 2013. LNCS-8166. Springer, Berlin, Heidelberg\n",
        "\n",
        "O[3] M. Theus: [Interactive Data Visualization using Mondrian](https://www.jstatsoft.org/article/view/v007i11/MondrianJSSV2.pdf). In Journal of Statistical Software 7 (11): 1–9.\n",
        "\n",
        "O[4] [Data Science Tools - Data Profiling with Pandas (Simple Exploratory Data Analysis) - YouTube ](https://www.youtube.com/watch?v=C7cmeEvUDfo)\n",
        "\n",
        "O[5] [Automated Data Profiling using Python Pandas (pandas-profiling) - YouTube ](https://www.youtube.com/watch?v=vsL8osE_0HM)\n",
        "\n",
        "O[6] [Using Pandas and Pandas Profiling to analyze data - YouTube ](https://www.youtube.com/watch?v=EaHWjkEPHr8)\n",
        "\n",
        "O[7] [Simple Exploratory Data Analysis with Pandas-Profiling Package Python - YouTube ](https://www.youtube.com/watch?v=773zrwAkmQ0)\n",
        "\n",
        "O[8] [6 Essential Data Visualization Python Libraries - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2020/03/6-data-visualization-python-libraries/?utm_source=feedburner&amp;utm_medium=email&amp;utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29)[ ](https://www.analyticsvidhya.com/blog/2020/03/6-data-visualization-python-libraries/?utm_source=feedburner&amp;utm_medium=email&amp;utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29)\n",
        "\n",
        "O[9] [How to Create Interactive Plots with Altair - Towards Data Science ](https://towardsdatascience.com/how-to-create-interactive-and-elegant-plot-with-altair-8dd87a890f2a)\n",
        "\n",
        "O[10] [7 Steps to Mastering Data Preparation for Machine Learning with Python — 2019 Edition ](https://www.kdnuggets.com/2019/06/7-steps-mastering-data-preparation-python.html)\n",
        "\n",
        "O[11] [Exploratory Data Analysis ](https://datascienceguide.github.io/exploratory-data-analysis)\n",
        "\n",
        "O[12] [Python Exploratory Data Analysis Tutorial - DataCamp ](https://www.datacamp.com/community/tutorials/exploratory-data-analysis-python)\n",
        "\n",
        "O[13] [Speed Up Your Exploratory Data Analysis With Pandas-Profiling ](https://towardsdatascience.com/speed-up-your-exploratory-data-analysis-with-pandas-profiling-88b33dc53625)\n",
        "\n",
        "M[14] Sz. Bozóki, A. Pataricza: [Extreme value analysis for capacity design](http://www.inderscience.com/info/inarticle.php?artid=95353). [Intl. Journal of Cloud Computing (IJCC)](http://www.inderscience.com/jhome.php?jcode=ijcc), [Vol. 7, No. 3/4, 2018](http://www.inderscience.com/info/inarticletoc.php?jcode=ijcc&amp;year=2018&amp;vol=7&amp;issue=3/4)"
      ]
    }
  ]
}